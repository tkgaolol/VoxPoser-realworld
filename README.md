# Robot Manipulation System

A language-guided robotic manipulation system that combines computer vision, natural language processing, and motion planning.

## ğŸ“‹ Prerequisites

### Required Libraries
```python
ultralytics        # YOLO object detection
torch             # Deep learning framework
opencv-python     # Computer vision
pyrealsense2      # RealSense camera interface
jupyter           # Notebook interface
openai            # LLM integration
plotly            # Data visualization
transforms3d      # 3D transformations
open3d            # 3D data processing 
numpy==1.26.4     # Numerical computing
pyserial            # Serial communication
transformers      
accelerate
```
you may need to run 
```bash
conda install -c conda-forge libstdcxx-ng 
```
if you encounter 
```bash
libGL error: MESA-LOADER: failed to open iris: /usr/lib/dri/iris_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)
libGL error: failed to load driver: iris
libGL error: MESA-LOADER: failed to open iris: /usr/lib/dri/iris_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)
libGL error: failed to load driver: iris
libGL error: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)
libGL error: failed to load driver: swrast
[Open3D WARNING] GLFW Error: GLX: Failed to create context: GLXBadFBConfig
[Open3D WARNING] Failed to create window
[Open3D WARNING] [DrawGeometries] Failed creating OpenGL window.
```

## ğŸ› ï¸ Setup Instructions
to be implemented
### Hardware Setup
1. Robot arm (TCP connection)
2. Gripper (Serial connection)
3. Depth Cameras:
   - RealSense camera
   - Orbbec Femto Mega camera (optional for multi-view)
4. Working platform

### Initial Configuration
1. Set up OpenAI API key
2. Install required submodules:
   ```bash
   # Clone submodules for vision components
   git submodule add https://github.com/hkchengrex/XMem.git real/toolbox/perceptron/XMem
   git submodule add https://github.com/orbbec/pyorbbecsdk.git real/toolbox/perceptron/femto_mega/pyorbbecsdk
   git submodule update --init --recursive
   ```
3. Camera Setup:
   - For RealSense: Install `pyrealsense2` package
   - For Orbbec Femto Mega: Install provided wheel file:
     ```bash
     pip install real/toolbox/perceptron/femto_mega/pyorbbecsdk-1.3.1-cp310-cp310-linux_x86_64.whl
     ```
4. Perform camera-to-robot calibration:
   ```bash
   python cal_Transform_mat.py
   ```
   - Press `Space` to capture current frame
   - Move robot arm to different positions (10 or more captures needed)
   - Press `Esc` or `Q` to exit
   - Copy the resulting transform matrix to `cam2robot.py`

   Note: For multi-camera setup, calibration needs to be performed for each camera.


## ğŸ“ Code Structure
```
.
â”œâ”€â”€ .vscode/                    # VS Code configuration
â”‚   â””â”€â”€ launch.json            # Debug configurations
â”œâ”€â”€ real/                      # Main project implementation
â”‚   â”œâ”€â”€ configs/               # Configuration files
â”‚   â”œâ”€â”€ model_weight/          # Pre-trained model weights
â”‚   â”œâ”€â”€ prompts/               # LLM prompt templates
â”‚   â”œâ”€â”€ toolbox/               # Core functionality modules
â”‚   â”‚   â”œâ”€â”€ perceptron/       # Vision and perception tools
â”‚   â”‚   â”‚   â”œâ”€â”€ XMem/         # Video object segmentation
â”‚   â”‚   â”‚   â”œâ”€â”€ femto_mega/   # Orbbec camera SDK
â”‚   â”‚   â”‚   â”œâ”€â”€ calibration/  # Camera-robot calibration
â”‚   â”‚   â”‚   â””â”€â”€ ...          # Other perception tools
â”‚   â”‚   â”œâ”€â”€ real_env.py       # Environment interface
â”‚   â”‚   â”œâ”€â”€ gripper.py        # Gripper control
â”‚   â”‚   â””â”€â”€ tcp_connection.py # Robot communication
â”‚   â”œâ”€â”€ LMP.py                # Language Model Programs
â”‚   â”œâ”€â”€ interfaces.py         # System interfaces
â”‚   â”œâ”€â”€ controllers.py        # Motion controllers
â”‚   â”œâ”€â”€ planners.py          # Motion planning
â”‚   â”œâ”€â”€ real.py              # Main execution
â”‚   â””â”€â”€ visualizers.py       # Visualization tools
â”œâ”€â”€ setup.py                  # Package configuration
â””â”€â”€ README.md                # Project documentation
```


## ğŸ”„ System Pipeline

### 1. Natural Language Command Processing
- Input example: "grab the mouse"
- Command parsing and task decomposition
- Spatial relationship interpretation

### 2. Perception System
1. **Object Detection**
   - YOLO-based RGB object detection
   - RealSense depth information acquisition
   - 3D coordinate conversion using:
     - RGB-D fusion
     - Camera intrinsics
     - Calibration transform matrix

2. **Advanced Vision Processing**
   - OWL-ViT for open-vocabulary detection
   - Segment Anything for mask generation
   - XMEM for mask tracking
   - Point cloud reconstruction

### 3. Motion Planning

#### Language Model Programs (LMP)
1. **Planner**: High-level command â†’ Sub-tasks
2. **Composer**: Sub-tasks â†’ Motion parameters
3. **Parser**: Object and spatial relationship interpretation

#### Value Maps
- Affordance mapping
- Obstacle avoidance
- End-effector velocity control
- Rotation constraints
- Gripper action planning

#### Trajectory Planning
- Voxel-based representation (100Ã—100Ã—100Ã—k)
- Euclidean distance transform for affordance
- Gaussian filtering for avoidance
- Cost optimization (2:1 affordance-avoidance weighting)

### 4. Execution and Control
1. Coordinate transformation
2. Real-time monitoring:
   - Collision detection
   - Gripper state
   - End-effector positioning
3. Dynamic replanning
4. Push motion parameterization:
   - Contact points
   - Push direction
   - Push distance




## ğŸ“ Documentation
### LLMs and Prompting
1. ä½¿ç”¨è‡ªå·±ç”Ÿæˆçš„ä»£ç é€’å½’è°ƒç”¨ LLM
2. å…¶ä¸­æ¯ä¸ªè¯­è¨€æ¨¡å‹ç¨‹åº (LMP) è´Ÿè´£ä¸€ä¸ªç‹¬ç‰¹çš„åŠŸèƒ½ï¼ˆä¾‹å¦‚ï¼Œå¤„ç†æ„ŸçŸ¥è°ƒç”¨ï¼‰
3. å¯¹äºæ¯ä¸ª LMPï¼Œæˆ‘ä»¬å°† 5-20 ä¸ªç¤ºä¾‹æŸ¥è¯¢å’Œç›¸åº”çš„å“åº”ä½œä¸ºæç¤ºçš„ä¸€éƒ¨åˆ†ã€‚
4. planner å°†ç”¨æˆ·æŒ‡ä»¤ä½œä¸ºè¾“å…¥ï¼ˆä¾‹å¦‚ï¼Œ"æ‰“å¼€æŠ½å±‰"ï¼‰å¹¶è¾“å‡ºä¸€ç³»åˆ—å­ä»»åŠ¡
5. composer æ¥æ”¶å­ä»»åŠ¡å¹¶è°ƒç”¨å…·æœ‰è¯¦ç»†è¯­è¨€å‚æ•°åŒ–çš„ç›¸å…³ä»·å€¼å›¾ LMP

### VLMs and Perception
1. è°ƒç”¨å¼€æ”¾è¯æ±‡æ£€æµ‹å™¨ OWL-ViT ä»¥è·å¾—è¾¹ç•Œæ¡†
2. å°†å…¶è¾“å…¥åˆ° Segment Anything ä¸­ä»¥è·å¾—mask
3. ä½¿ç”¨è§†é¢‘è·Ÿè¸ªå™¨ XMEM è·Ÿè¸ªmask
4. è·Ÿè¸ªçš„maskä¸ RGB-D ä¿¡æ¯ ä¼šä¸€èµ·ä½¿ç”¨ä»¥é‡å»ºå¯¹è±¡/éƒ¨åˆ†ç‚¹äº‘

### Value Map Composition
1. affordance, avoidance, endeffector velocity, end-effector rotation, and gripper action
2. æ¯ç§ç±»å‹ä½¿ç”¨ä¸åŒçš„ LMPï¼šæ¥æ”¶æŒ‡ä»¤å¹¶è¾“å‡ºå½¢çŠ¶ä¸º (100, 100, 100, k) çš„ä½“ç´ å›¾ï¼Œå…¶ä¸­ k å¯¹äºæ¯ä¸ªä»·å€¼å›¾éƒ½ä¸åŒï¼ˆä¾‹å¦‚ï¼Œå¯ä¾›æ€§å’Œé¿å…æ€§çš„ k = 1ï¼Œå› ä¸ºå®ƒæŒ‡å®šæˆæœ¬ï¼Œè€Œæ—‹è½¬çš„ k = 4ï¼Œå› ä¸ºå®ƒæŒ‡å®š SO(3)ï¼‰
3. æ¬§å‡ é‡Œå¾—è·ç¦»å˜æ¢åº”ç”¨äºaffordanceï¼Œå¹¶å°†é«˜æ–¯æ»¤æ³¢å™¨åº”ç”¨äºavoidance

### Motion Planner
1. consider only affordance and avoidance maps in the planner optimization
2. greedy searchæ‰¾åˆ°ä¸€ç³»åˆ—æ— ç¢°æ’çš„æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®
3. åœ¨æ¯ä¸ªä½ç½®ï¼Œæˆ‘ä»¬é€šè¿‡å‰©ä½™çš„å€¼å›¾ï¼ˆä¾‹å¦‚ï¼Œæ—‹è½¬å›¾ã€é€Ÿåº¦å›¾ï¼‰å¼ºåˆ¶å…¶ä»–å‚æ•°åŒ–
4. The cost map used by the motion planner is computed as the negative of the weighted sum
of normalized affordance and avoidance maps with weights 2 and 1

### Dynamics Model
1. ä¸ä½¿ç”¨ç¯å¢ƒåŠ¨åŠ›å­¦æ¨¡å‹ï¼ˆå³ï¼Œå‡è®¾åœºæ™¯æ˜¯é™æ€çš„ï¼‰
2. åœ¨æ¯ä¸€æ­¥éƒ½é‡æ–°è§„åˆ’ä»¥è€ƒè™‘æœ€æ–°çš„è§‚å¯Ÿç»“æœ
3. åªç ”ç©¶ç”±æ¥è§¦ç‚¹ã€æ¨åŠ¨æ–¹å‘å’Œæ¨åŠ¨è·ç¦»å‚æ•°åŒ–çš„å¹³é¢æ¨åŠ¨æ¨¡å‹



## ğŸ” TODO
1. å¯ä»¥çœ‹ä¸€ä¸‹æ–‡ç« æœ€åçš„api å’Œ promptsçš„å†™æ³•
2. å…·ä½“å®ç°è¦çœ‹æ–‡ç« ç¬¬å››éƒ¨åˆ†å¼€å¤´
3. è”åˆrosä¸€èµ·æå—ï¼Ÿ
4. å…¨è‡‚é¿éšœè§„åˆ’
5. è®°å¾—å‡†å¤‡æ¿å­ æŠŠå·¥ä½œå¹³å°åˆ†å‰²å¥½
6. æŠ“å–å§¿åŠ¿,å¤¹çˆªå¤¹å¤šå°‘ï¼ŒåŠ›ç»™å¤šå°‘ FoundationPoseg
7. å¤šæ•´ä¸ªæ·±åº¦ç›¸æœºï¼Ÿç”¨kinectï¼Œåˆå¹¶ç‚¹äº‘ï¼Œæ ‡å®šã€‚ä¸ç„¶ä»–æå–çš„ç›®æ ‡ä½ç½®æ˜¯è¡¨é¢ä¸æ˜¯ä¸­å¿ƒ
8. å¯èƒ½è¦é‡æ–°çœ‹ä¸€ä¸‹æ ‡å®šæ€ä¹ˆåšï¼Œå› ä¸ºå¤¹çˆªçš„ä½ç½®ä¸æ˜¯æœºæ¢°è‡‚æœ«ç«¯çš„ä½ç½®
9. æµ‹è¯•æ‰€æœ‰demo æ¢ä¸ªç‰©ä½“èƒ½æ£€æµ‹åˆ°çš„

## Implementation notes
1. cv2.imshow will freeze if you import av simultaneously(because ffmpeg used by av and opencv is not compatible)
2. numpy 1.26.4 is required for the femto_mega camera

## Acknowledgements
This project is built on top of [VoxPoser](https://github.com/huangwl18/VoxPoser).
